{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e41f3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Pavlo\\anaconda3\\envs\\testmlflow\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import lightning as L\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a6195f-111f-426e-a1dc-799ab6df1821",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Data has already been prepared for you\n",
    "train = np.load('HA3_train_set.npz')\n",
    "test = np.load('HA3_test_set.npz')\n",
    "X_train = train['x_train']\n",
    "y_train = train['y_train']\n",
    "X_test = test['x_test']\n",
    "y_test = test['y_test']\n",
    "\n",
    "# Step 1: Convert to tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.long)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Step 2: Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38134cf5-07ba-4876-b1ac-8d0d24ae5717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape: (25000, 500)\n",
      "y_train.shape: (25000,)\n",
      "X_test.shape: (17500, 500)\n",
      "y_test.shape: (17500,)\n"
     ]
    }
   ],
   "source": [
    "print(\"X_train.shape:\", X_train.shape)\n",
    "print(\"y_train.shape:\", y_train.shape)\n",
    "print(\"X_test.shape:\", X_test.shape)\n",
    "print(\"y_test.shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c65d67a5-1076-41be-a3df-1b238c2cbd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recurrent neural network (many-to-one)\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.dropout = nn.Dropout(0.01)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, num_classes)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        out, (final_hidden_state, final_cell_state) = self.lstm(embedded)\n",
    "        #out = self.layer_norm(out[:, -1, :])\n",
    "        #out = self.dropout(out)\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        #out = F.relu(out)\n",
    "        #out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8700726-3734-4a4d-8125-f6dcb4ccd6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, learning_rate, train_loader, test_loader):\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.NLLLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    best_accuracy = 0.0\n",
    "    global_step = 0\n",
    "    no_improvement_counter = 0\n",
    "    patience = 10\n",
    "\n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (comments, labels) in enumerate(train_loader):\n",
    "            global_step += 1\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(comments)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            mlflow.log_metric(\"loss\", loss.item(), step = global_step)\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "                \n",
    "                # Test the model\n",
    "                with torch.no_grad():\n",
    "                    correct = 0\n",
    "                    total = 0\n",
    "                    for comments, labels in test_loader:\n",
    "                        outputs = model(comments)\n",
    "                        _, predicted = torch.max(outputs.data, 1)\n",
    "                        total += labels.size(0)\n",
    "                        correct += (predicted == labels).sum().item()\n",
    "                        accuracy = 100 * correct / total\n",
    "                    mlflow.log_metric(\"Test accuracy vs number of training epochs\", 100 * correct / total, step = global_step)\n",
    "\n",
    "                # Save the best model\n",
    "                if accuracy > best_accuracy:\n",
    "                    best_accuracy = accuracy\n",
    "                    mlflow.pytorch.log_model(pytorch_model=model, artifact_path=\"best-torch-rnn-model\")\n",
    "                    no_improvement_counter = 0\n",
    "                else:\n",
    "                    no_improvement_counter += 1\n",
    "\n",
    "                print('Test Accuracy of the model on test comments: {} %'.format(100 * correct / total))\n",
    "\n",
    "                # Early stopping criterion\n",
    "                if no_improvement_counter >= patience:\n",
    "                    print(f'Stopping early after {patience} evaluations without improvement.')\n",
    "                    return best_accuracy\n",
    "                \n",
    "    return best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beb24792-22f3-444e-b9bb-f6828b597ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "\n",
    "    num_layers = 1 ##trial.suggest_int('num_layers', 1, 2)\n",
    "    hidden_size = 15 ##trial.suggest_int('hidden_size', 1, 30)\n",
    "    batch_size = 758 ##trial.suggest_int('batch_size', 64, 1028)\n",
    "    embedding_dim = 20 ##trial.suggest_int('embedding_dim', 1, 64)\n",
    "    learning_rate = 0.036 ##trial.suggest_float('learning_rate', 0.001, 0.1)\n",
    "\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    num_classes = 2  # Fixed number of classes\n",
    "    num_epochs = 100  # For faster trials, you might want to use more epochs in actual training\n",
    "    vocab_size = tokenizer.vocab_size\n",
    "\n",
    "    with mlflow.start_run():\n",
    "        mlflow.log_param(\"num_hidden_layers\", num_layers)\n",
    "        mlflow.log_param(\"hidden_size\", hidden_size)\n",
    "        mlflow.log_param(\"batch_size\", batch_size)\n",
    "        mlflow.log_param(\"epochs\", num_epochs)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"embedding_dim\", embedding_dim)\n",
    "\n",
    "        model = RNN(embedding_dim, vocab_size, hidden_size, num_layers, num_classes)\n",
    "        print('----------------------------------------------------------')\n",
    "        best_accuracy = train_model(model, num_epochs, learning_rate, train_loader, test_loader)  # Modify train_model to return accuracy\n",
    "\n",
    "        mlflow.log_metric(\"best_accuracy\", best_accuracy)\n",
    "\n",
    "    return best_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c0240da2-5ba8-4f68-a906-c0d82480a5de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-06 11:23:53,594] A new study created in memory with name: no-name-dff43fb0-e343-4010-8073-a20672301888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------\n",
      "Epoch [1/100], Step [10/33], Loss: 0.6798\n",
      "Test Accuracy of the model on test comments: 53.94857142857143 %\n",
      "Epoch [1/100], Step [20/33], Loss: 0.6791\n",
      "Test Accuracy of the model on test comments: 56.23428571428571 %\n",
      "Epoch [1/100], Step [30/33], Loss: 0.6717\n",
      "Test Accuracy of the model on test comments: 61.12 %\n",
      "Epoch [2/100], Step [10/33], Loss: 0.5792\n",
      "Test Accuracy of the model on test comments: 68.46285714285715 %\n",
      "Epoch [2/100], Step [20/33], Loss: 0.4986\n",
      "Test Accuracy of the model on test comments: 75.56 %\n",
      "Epoch [2/100], Step [30/33], Loss: 0.4410\n",
      "Test Accuracy of the model on test comments: 80.48571428571428 %\n",
      "Epoch [3/100], Step [10/33], Loss: 0.2527\n",
      "Test Accuracy of the model on test comments: 83.55428571428571 %\n",
      "Epoch [3/100], Step [20/33], Loss: 0.2702\n",
      "Test Accuracy of the model on test comments: 85.86857142857143 %\n",
      "Epoch [3/100], Step [30/33], Loss: 0.2278\n",
      "Test Accuracy of the model on test comments: 83.33714285714285 %\n",
      "Epoch [4/100], Step [10/33], Loss: 0.1658\n",
      "Test Accuracy of the model on test comments: 86.2 %\n",
      "Epoch [4/100], Step [20/33], Loss: 0.1444\n",
      "Test Accuracy of the model on test comments: 86.52 %\n",
      "Epoch [4/100], Step [30/33], Loss: 0.1666\n",
      "Test Accuracy of the model on test comments: 86.42857142857143 %\n",
      "Epoch [5/100], Step [10/33], Loss: 0.0826\n",
      "Test Accuracy of the model on test comments: 85.65142857142857 %\n",
      "Epoch [5/100], Step [20/33], Loss: 0.1295\n",
      "Test Accuracy of the model on test comments: 85.65714285714286 %\n",
      "Epoch [5/100], Step [30/33], Loss: 0.1267\n",
      "Test Accuracy of the model on test comments: 85.90857142857143 %\n",
      "Epoch [6/100], Step [10/33], Loss: 0.0640\n",
      "Test Accuracy of the model on test comments: 86.04571428571428 %\n",
      "Epoch [6/100], Step [20/33], Loss: 0.0743\n",
      "Test Accuracy of the model on test comments: 85.48 %\n",
      "Epoch [6/100], Step [30/33], Loss: 0.0909\n",
      "Test Accuracy of the model on test comments: 86.17714285714285 %\n",
      "Epoch [7/100], Step [10/33], Loss: 0.0394\n",
      "Test Accuracy of the model on test comments: 86.13714285714286 %\n",
      "Epoch [7/100], Step [20/33], Loss: 0.0510\n",
      "Test Accuracy of the model on test comments: 85.88 %\n",
      "Epoch [7/100], Step [30/33], Loss: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-06-06 11:45:21,196] Trial 0 finished with value: 86.52 and parameters: {}. Best is trial 0 with value: 86.52.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on test comments: 85.72571428571429 %\n",
      "Stopping early after 10 evaluations without improvement.\n",
      "Best trial: 86.52\n",
      "Best hyperparameters: {}\n"
     ]
    }
   ],
   "source": [
    "# Create a study and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=1)\n",
    "\n",
    "# Log the best hyperparameters\n",
    "best_trial = study.best_trial\n",
    "print(f'Best trial: {best_trial.value}')\n",
    "print(f'Best hyperparameters: {best_trial.params}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0b7c25-89ad-4acc-851a-76d793019e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
